# online_rag_qa_batch_parameter.yaml.template
# Template for batch QA processing parameters
# The script will replace placeholders with actual values at runtime

benchmark:
  benchmark:
    name: nq
    path: __QUESTIONS_PATH__
    key_map:
      q_ls: question
      gt_ls: golden_answers
    shuffle: false
    seed: 42
    limit: -1

retriever:
  model_name_or_path: __EMBEDDING_MODEL_PATH__
  backend_configs:
    infinity:
      bettertransformer: false
      pooling_method: auto
      model_warmup: false
      trust_remote_code: true
    sentence_transformers:
      trust_remote_code: true
      sentence_transformers_encode:
        normalize_embeddings: false
        encode_chunk_size: 256
        q_prompt_name: query
        psg_prompt_name: document
        psg_task: null
        q_task: null
    openai:
      model_name: qwen3-embedding-0.6b
      base_url: http://0.0.0.0:65503/v1
      api_key: abc
    bm25:
      lang: en
      save_path: index/bm25
  batch_size: 16
  corpus_path: __CHUNKS_PATH__
  gpu_ids: "0"
  is_multimodal: false
  backend: sentence_transformers
  index_backend: __INDEX_BACKEND__
  index_backend_configs:
    faiss:
      index_use_gpu: true
      index_chunk_size: 10000
      index_path: __INDEX_PATH__
    milvus:
      uri: __MILVUS_URI__
      token: null
      id_field_name: id
      vector_field_name: vector
      text_field_name: contents
      id_max_length: 64
      text_max_length: 60000
      metric_type: IP
      index_params:
        index_type: AUTOINDEX
        metric_type: IP
      search_params:
        metric_type: IP
        params: {}
      index_chunk_size: 1000
  is_demo: false
  collection_name: __COLLECTION_NAME__
  top_k: 5
  query_instruction: ""

reranker:
  model_name_or_path: __RERANKER_MODEL_PATH__
  backend_configs:
    infinity:
      bettertransformer: false
      pooling_method: auto
      device: cuda
      model_warmup: false
      trust_remote_code: true
    sentence_transformers:
      device: cuda
      trust_remote_code: true
    openai:
      model_name: text-embedding-3-small
      base_url: https://api.openai.com/v1
      api_key: abc
  batch_size: 16
  gpu_ids: 0
  backend: sentence_transformers
  top_k: 5
  query_instruction: ""

generation:
  backend_configs:
    vllm:
      model_name_or_path: __GENERATION_MODEL_PATH__
      gpu_ids: 0
      gpu_memory_utilization: 0.85
      max_model_len: 8192
      dtype: auto
      trust_remote_code: true
    openai:
      model_name: __SERVED_MODEL_NAME__
      base_url: __VLLM_BASE_URL__
      api_key: abc
      concurrency: 8
      retries: 3
      base_delay: 1.0
    hf:
      model_name_or_path: /home/NagaiYoru/LLM_model/MiniCPM4-8B
      gpu_ids: 2,3
      trust_remote_code: true
      batch_size: 8
  sampling_params:
    temperature: 0.7
    top_p: 0.8
    max_tokens: 2048
  extra_params:
    chat_template_kwargs:
      enable_thinking: false
  backend: openai
  system_prompt: "__SYSTEM_PROMPT__"

prompt:
  template: prompt/qa_boxed.jinja

custom: {}
